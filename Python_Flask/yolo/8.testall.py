import os
import cv2
import time
from ultralytics import YOLO
import numpy as np
import torch

# è‡ªå‹•é¸æ“‡è¨­å‚™
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è¨­å‚™ï¼š{device}")

# æŒ‡å®šå½±ç‰‡è³‡æ–™å¤¾
video_folder = r"D:\åœ–\garbage\å½±ä¸‰"
# è¼‰å…¥ YOLO æ¨¡å‹
#model_path = r"Z:\å°ˆé¡Œ\datatest\train7\weights\best.pt"
model_path = r"Z:\å°ˆé¡Œ\ç´…æ¨“åƒåœ¾\train12\weights\best.pt"
#class_names = ['foot', 'shoe']
class_names = ['Bottle', 'Tissue','Plastic']
model = YOLO(model_path)
# æ”¯æ´çš„å½±ç‰‡æ ¼å¼
video_extensions = ['.mp4', '.avi', '.mov', '.mkv']

# è‡ªå‹•åˆ—å‡ºè³‡æ–™å¤¾å…§æ‰€æœ‰å½±ç‰‡
video_sources = [
    os.path.join(video_folder, f) for f in os.listdir(video_folder)
    if os.path.splitext(f)[1].lower() in video_extensions
]

# ç¢ºèªæ‰¾åˆ°å½±ç‰‡
if not video_sources:
    print("âŒ è³‡æ–™å¤¾è£¡æ²’æœ‰æ‰¾åˆ°ä»»ä½•å½±ç‰‡")
    exit()

print(f"æ‰¾åˆ° {len(video_sources)} éƒ¨å½±ç‰‡")
for idx, video in enumerate(video_sources):
    print(f"{idx+1}. {video}")

video_index = 0  # ç›®å‰æ’­æ”¾åˆ°ç¬¬å¹¾å€‹å½±ç‰‡



# åˆå§‹åŒ–è¨ˆæ•¸
frame_count = 0
total_fps = 0
prev_time = time.time()

# é¡è‰²è¨­å®š
def get_color(class_name):
    color_dict = {'user': (0, 255, 0), 'nouser': (0, 0, 255)}
    return color_dict.get(class_name, (0, 255, 255))

# æ‰“é–‹ç¬¬ä¸€å€‹å½±ç‰‡
cap = cv2.VideoCapture(video_sources[video_index])
if not cap.isOpened():
    print(f"âŒ ç„¡æ³•æ‰“é–‹å½±ç‰‡ {video_sources[video_index]}")
    exit()

fps_video = cap.get(cv2.CAP_PROP_FPS)
frame_interval = 1 / fps_video
print(f"\nâ–¶ï¸ æ’­æ”¾å½±ç‰‡ï¼š{video_sources[video_index]}")
print(f"å½±ç‰‡åŸå§‹FPSï¼š{fps_video}")

while True:
    start_time = time.time()

    ret, frame = cap.read()
    if not ret:
        # é€™éƒ¨å½±ç‰‡æ’­æ”¾å®Œï¼Œæ›ä¸‹ä¸€éƒ¨
        video_index += 1
        if video_index >= len(video_sources):
            print("ğŸ¬ å½±ç‰‡å…¨éƒ¨æ’­æ”¾å®Œç•¢")
            break
        cap.release()
        cap = cv2.VideoCapture(video_sources[video_index])
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•æ‰“é–‹å½±ç‰‡ {video_sources[video_index]}")
            break
        fps_video = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = 1 / fps_video
        print(f"\nâ–¶ï¸ æ’­æ”¾å½±ç‰‡ï¼š{video_sources[video_index]}")
        continue

    # YOLO åµæ¸¬
    results = model(frame, iou=0.3, conf=0.5, device=device)
    result = results[0]
    bboxes = np.array(result.boxes.xyxy.cpu(), dtype="int")
    classes = np.array(result.boxes.cls.cpu(), dtype="int")
    confs = result.boxes.conf.cpu().tolist()

    for cls, bbox, conf in zip(classes, bboxes, confs):
        (x, y, x2, y2) = bbox
        class_name = class_names[cls]
        color = get_color(class_name)

        cv2.rectangle(frame, (x, y), (x2, y2), color, 2)
        cv2.putText(frame, f"{class_name} ({conf:.2f})", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

    # FPSè¨ˆç®—
    current_time = time.time()
    actual_fps = 1 / (current_time - prev_time)
    prev_time = current_time

    frame_count += 1
    total_fps += actual_fps

    cv2.putText(frame, f"FPS: {actual_fps:.2f}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow("YOLOv11 Detection", frame)

    processing_time = time.time() - start_time
    delay = max(int((frame_interval - processing_time) * 1000), 1)
    key = cv2.waitKey(delay)
    if key == 27:  # ESC
        break

# çµ±è¨ˆ
if frame_count > 0:
    average_fps = total_fps / frame_count
    print(f"\nğŸ“Š ç¸½å¹€æ•¸ï¼š{frame_count}ï¼Œå¹³å‡ FPSï¼š{average_fps:.2f}")
else:
    print("âš ï¸ æ²’æœ‰è®€å–åˆ°ä»»ä½•å¹€")

# æ”¶å°¾
cap.release()
cv2.destroyAllWindows()
